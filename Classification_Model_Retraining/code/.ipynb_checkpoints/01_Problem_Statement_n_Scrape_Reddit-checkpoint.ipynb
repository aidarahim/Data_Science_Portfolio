{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01\n",
    "\n",
    "# Problem Statement: How well does a classification model hold up over time?\n",
    "\n",
    "I am interested in exploring how a successful classification model holds up over time. If a model is built based on a dataset collected at a specific point in time, how far into the future can it be applied, and still be successful (accurate)? Where success is defined as performing better than the baseline accuracy. It would be a bonus if I can identify whether covid specifically had an impact, by creating my model from posts gathered from Sept 2019, and applying the model to posts gathered during subsequent months. In order to inject an added degree of difficulty to this problem, the subreddits need to have similar content. And in order to successfully fulfill the requirement to collect data over time, the subreddit memberships need to be very high, to generate enough volume. I am additionally curious about how generalizable the model will be when applied to posts from other subreddits.\n",
    "\n",
    "I start with 2 categories: practical here's how you do stuff (r/LifeProTips), and deeper existential stuff (r/Showerthoughts). A big advantage about using these subreddits is that they are among the largest subreddits, with memberships ~20M. 1000 posts are extracted from each subreddit, starting at midnight on the 20th of each month.\n",
    "\n",
    "Notebook sequence:\n",
    "1. Webscraping\n",
    "2. Exploratory Data Analysis\n",
    "3. Logistic Regression Model\n",
    "4. Random Forest, Extra Trees, Support Vector Machine Models\n",
    "5. Apply Models to Datasets & Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping Reddit using API\n",
    "\n",
    "In this notebook, posts are initially scraped from 2 subreddits, 'LifeProTips' (19M members) and 'Showerthoughts' (22M members) using the Pushshift API. For each subreddit, 1000 posts are extracted starting from the 20th day of each month (and moving backwards), over a 17-month period. Each set of 1000 posts comes from approximately 6 days, and is saved in its own dataframe.\n",
    "\n",
    "The training/test set for modeling (notebook #3) will be created from a dataset grabbed in Sept 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab posts from these subreddits: 'LifeProTips' & 'Showerthoughts'\n",
    "# Extract 1000 posts starting from the 20th day of each month, midnight (and moving backwards), for a 17-month period (starting before Covid)\n",
    "# Every 100 posts covers ~12 hours, so 1000 posts spans ~6 days \n",
    "# Place each 1000 posts in its own df\n",
    "# Use the posts from Sept 20, 2020 as the training set (train on a dataset away from the 2 known big events: covid and perseverance landing)\n",
    "# Test all other sets of posts\n",
    "# Compare accuracy over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "\n",
    "url = \"https://api.pushshift.io/reddit/search/submission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Epoch Converter to generate Epoch timestamps for all months\n",
    "# https://www.epochconverter.com/\n",
    "\n",
    "# timestamps from Oct 2019 - Feb 2021\n",
    "timestamp_all = [1571529600, 1574208000, 1576800000, 1579478400, \n",
    "                1582156800, 1584662400, 1587340800, 1589932800, 1592611200, 1595203200,\n",
    "                1597881600, 1600560000, 1603152000, 1605830400, 1608422400,\n",
    "                1611100800, 1613779200]\n",
    "\n",
    "# create 17 dataframes with the 1st entry being 'init'\n",
    "\n",
    "dict = {'subreddit': 'init', 'selftext': 'init', 'title': 'init'}\n",
    "\n",
    "# instantiate empty dataframes\n",
    "df1_name = []\n",
    "df2_name = []\n",
    "\n",
    "subr = ['LifeProTips','Showerthoughts'] #19M, 22M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape 'LifeProTips'\n",
    "for i in range(len(timestamp_all)):\n",
    "    \n",
    "    # generate dataframe name\n",
    "    df1_name.append(f\"df_{i+1}_{subr[0]}\")\n",
    "    \n",
    "    # create dataframe with 'init' values\n",
    "    # Google: python create variable name from string\n",
    "    # https://www.daniweb.com/programming/software-development/threads/111526/setting-a-string-as-a-variable-name\n",
    "    vars()[df1_name[i]] = pd.DataFrame([dict])\n",
    "    timestamp = timestamp_all[i]\n",
    "    \n",
    "    # loop through 10 times to collect 1000 posts in total\n",
    "    for j in range(10):\n",
    "        params = {\n",
    "            'subreddit':subr[0],\n",
    "            'size': 100,\n",
    "            'before': timestamp\n",
    "        }\n",
    "        \n",
    "        res = requests.get(url,params)\n",
    "        res.status_code\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df = pd.DataFrame(posts)\n",
    "\n",
    "        # concatenate dataframes\n",
    "        vars()[df1_name[i]] = pd.concat([vars()[df1_name[i]],df[['subreddit','selftext','title']]])\n",
    "        \n",
    "        # grab timestamp from most recent batch of 100 posts\n",
    "        timestamp = posts[99]['created_utc']\n",
    "        # check that the 'for' loop is grabbing unique posts\n",
    "        # print(vars()[df1_name[i]]['title'].iloc[(vars()[df1_name[i]].shape[0])-1])\n",
    "        time.sleep(3) # rest for 3 seconds before doing this again\n",
    "    \n",
    "    # drop the first row\n",
    "    vars()[df1_name[i]] = vars()[df1_name[i]].iloc[1:]\n",
    "    \n",
    "    # save df\n",
    "    vars()[df1_name[i]].to_csv(f'../data/raw/{subr[0]}_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape 'Showerthoughts'\n",
    "for i in range(len(timestamp_all)):\n",
    "    df2_name.append(f\"df_{i+1}_{subr[1]}\")\n",
    "    vars()[df2_name[i]] = pd.DataFrame([dict])\n",
    "    timestamp = timestamp_all[i]\n",
    "    \n",
    "    for j in range(10):\n",
    "        params = {\n",
    "            'subreddit':subr[1],\n",
    "            'size': 100,\n",
    "            'before': timestamp\n",
    "        }\n",
    "        \n",
    "        res = requests.get(url,params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df = pd.DataFrame(posts)\n",
    "        vars()[df2_name[i]] = pd.concat([vars()[df2_name[i]],df[['subreddit','selftext','title']]])\n",
    "        timestamp = posts[99]['created_utc']\n",
    "        # check that the 'for' loop is grabbing unique posts\n",
    "        # print(vars()[df2_name[i]]['title'].iloc[(vars()[df2_name[i]].shape[0])-1])\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # drop the first row\n",
    "    vars()[df2_name[i]] = vars()[df2_name[i]].iloc[1:]\n",
    "    \n",
    "    # save df\n",
    "    vars()[df2_name[i]].to_csv(f'../data/raw/{subr[1]}_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract 1000 posts per subreddit\n",
    "\n",
    "Extract posts from Sept 2019 into separate files, with a different file naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of other potential subreddits\n",
    "# subr = ['financialindependence','FinancialPlanning'] #866k, 203k\n",
    "# subr = ['books','news'] #19.1M, 22.9M\n",
    "# subr = ['stocks','gardening'] #2.4M, 3.5M\n",
    "# subr = ['boardgames','gaming'] #3.4M, 29.4M (all games inc. boardgames, except sports)\n",
    "# subr = ['todayilearned','nottheonion'] #25M, 18.9M\n",
    "# subr = ['space','science'] #18.2M, 26M\n",
    "\n",
    "# timestamp for 9/20/2019, midnight\n",
    "timestamp = [1568937600]\n",
    "filename = ['df_' + subr[0], 'df_' + subr[1]]\n",
    "\n",
    "vars()[filename[0]] = pd.DataFrame([dict])\n",
    "vars()[filename[1]] = pd.DataFrame([dict])\n",
    "\n",
    "##################################\n",
    "####### FIRST DATAFRAME ##########\n",
    "##################################\n",
    "\n",
    "for j in range(10):\n",
    "    params = {\n",
    "        'subreddit':subr[0],\n",
    "        'size': 100,\n",
    "        'before': timestamp\n",
    "    }\n",
    "\n",
    "    res = requests.get(url,params)\n",
    "    res.status_code\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    df = pd.DataFrame(posts)\n",
    "    vars()[filename[0]] = pd.concat([vars()[filename[0]],df[['subreddit','selftext','title']]])\n",
    "    timestamp = posts[99]['created_utc']\n",
    "    time.sleep(3)\n",
    "\n",
    "# drop the first row\n",
    "vars()[filename[0]] = vars()[filename[0]].iloc[1:]\n",
    "\n",
    "# save df\n",
    "vars()[filename[0]].to_csv(f'../data/{filename[0]}.csv', index=False)\n",
    "\n",
    "##################################\n",
    "####### SECOND DATAFRAME #########\n",
    "##################################\n",
    "\n",
    "for j in range(10):\n",
    "    params = {\n",
    "        'subreddit':subr[1],\n",
    "        'size': 100,\n",
    "        'before': timestamp\n",
    "    }\n",
    "\n",
    "    res = requests.get(url,params)\n",
    "    res.status_code\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    df = pd.DataFrame(posts)\n",
    "    vars()[filename[1]] = pd.concat([vars()[filename[1]],df[['subreddit','selftext','title']]])\n",
    "    timestamp = posts[99]['created_utc']\n",
    "    time.sleep(3)\n",
    "\n",
    "# drop the first row\n",
    "vars()[filename[1]] = vars()[filename[1]].iloc[1:]\n",
    "\n",
    "# save df\n",
    "vars()[filename[1]].to_csv(f'../data/{filename[1]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
